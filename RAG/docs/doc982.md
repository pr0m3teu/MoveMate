bugfix(consistency): Correctly index Effects V1 input objects by amnn Â· Pull Request #23851 Â· MystenLabs/sui Â· GitHub



[Skip to content](#start-of-content)







## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FMystenLabs%2Fsui%2Fpull%2F23851)

Appearance settings

Search or jump to...


# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.


Include my email address so I can be contacted

Cancel
 Submit feedback





# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

Cancel
 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FMystenLabs%2Fsui%2Fpull%2F23851)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fpull_requests_fragments%2Fpull_request_layout&source=header-repo&source_repo=MystenLabs%2Fsui)

Appearance settings

Resetting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.
 


Dismiss alert

{{ message }}

[MystenLabs](/MystenLabs) 
/
**[sui](/MystenLabs/sui)**
Public

* [Notifications](/login?return_to=%2FMystenLabs%2Fsui) You must be signed in to change notification settings
* [Fork
  11.7k](/login?return_to=%2FMystenLabs%2Fsui)
* [Star
   7.5k](/login?return_to=%2FMystenLabs%2Fsui)

# bugfix(consistency): Correctly index Effects V1 input objects #23851

New issue

 

**Have a question about this project?** Sign up for a free GitHub account to open an issue and contact its maintainers and the community.

[Sign up for GitHub](/signup?return_to=%2FMystenLabs%2Fsui%2Fissues%2Fnew%2Fchoose)

By clicking â€œSign up for GitHubâ€, you agree to our [terms of service](https://docs.github.com/terms) and
[privacy statement](https://docs.github.com/privacy). Weâ€™ll occasionally send you account related emails.

Already on GitHub?
[Sign in](/login?return_to=%2FMystenLabs%2Fsui%2Fissues%2Fnew%2Fchoose)
to your account

[Jump to bottom](#issue-comment-box)

Merged

[amnn](/amnn)
merged 1 commit into
[main](/MystenLabs/sui/tree/main "MystenLabs/sui:main")
from
[amnn/cos-coin](/MystenLabs/sui/tree/amnn/cos-coin "MystenLabs/sui:amnn/cos-coin")








Oct 6, 2025

Merged

# [bugfix(consistency): Correctly index Effects V1 input objects](#top) #23851

[amnn](/amnn)
merged 1 commit into
[main](/MystenLabs/sui/tree/main "MystenLabs/sui:main")
from
[amnn/cos-coin](/MystenLabs/sui/tree/amnn/cos-coin "MystenLabs/sui:amnn/cos-coin")








Oct 6, 2025

### Uh oh!

There was an error while loading. Please reload this page.

## Conversation

This file contains hidden or bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

[Show hidden characters]({{ revealButtonHref }})








[![@amnn](https://avatars.githubusercontent.com/u/332275?s=80&v=4)](/amnn)

Copy link

Contributor

### @amnn **[amnn](/amnn)** commented [Oct 6, 2025](#issue-3486765648)

## Description

In Effects V1, the input state of modified objects doesn't include the digest. The consistent store expected the input state to have both the version and the digest, so it wasn't correctly deleting the previous version of the modified object.

## Test plan

New regression test:

```move
$ cargo nextest run            \
  -p sui-indexer-alt-e2e-tests \
  --test consistent_store_list_owned_objects_tests
```

---

## Release notes

Check each box that your changes affect. If none of the boxes relate to your changes, release notes aren't required.

For each box you select, include information after the relevant heading that describes the impact of your changes that a user might notice and any actions they must take to implement updates.

* Protocol:
* Nodes (Validators and Full nodes):
* gRPC:
* JSON-RPC:
* GraphQL:
* CLI:
* Rust SDK:

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

All reactions

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&v=4)](/amnn)

`bugfix(consistency): Correctly index Effects V1 input objects`

 â€¦

`4e04889`

```move
## Description

In Effects V1, the input state of modified objects doesn't include the
digest. The consistent store expected the input state to have both the
version and the digest, so it wasn't correctly deleting the previous
version of the modified object.

## Test plan

New regression test:

```
$ cargo nextest run            \
  -p sui-indexer-alt-e2e-tests \
  --test consistent_store_list_owned_objects_tests
```
```

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)
[amnn](/amnn)
self-assigned this
[Oct 6, 2025](#event-20112374124)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)
[amnn](/amnn)
requested a review
from a team
as a [code owner](/MystenLabs/sui/blob/319fe085a2387747dc19d798895d77bd6d594063/.github/CODEOWNERS#L20)
[October 6, 2025 11:13](#event-20112374262)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)
[amnn](/amnn)
requested review from
[emmazzz](/emmazzz),
[evan-wall-mysten](/evan-wall-mysten),
[henryachen](/henryachen),
[tpham-mysten](/tpham-mysten) and
[wlmyng](/wlmyng)
[October 6, 2025 11:13](#event-20112374371)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)
[amnn](/amnn)
[temporarily deployed](https://github.com/MystenLabs/sui/actions/runs/18278977944/job/52037456321)
to
sui-typescript-aws-kms-test-env
[October 6, 2025 11:13](#event-20112376027) â€” with ![](https://avatars.githubusercontent.com/in/15368?s=40&u=167a342ed94d2a713daf64a8b476ead2cebe1852&v=4)
[GitHub Actions](https://github.com/apps/github-actions)

Inactive

[![@vercel](https://avatars.githubusercontent.com/in/8329?s=80&v=4)](/apps/vercel)

Copy link

### **[vercel](/apps/vercel) bot** commented [Oct 6, 2025](#issuecomment-3371089992) â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page.

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| The latest updates on your projects. Learn more about [Vercel for GitHub](https://vercel.link/github-learn-more).   | Project | Deployment | Preview | Comments | Updated (UTC) | | --- | --- | --- | --- | --- | | [sui-docs](https://vercel.com/sui-foundation/sui-docs) | [Ready](https://camo.githubusercontent.com/2d058aa4937109084c3b00dc259081ef9f01257a8504a292a286041635575d96/68747470733a2f2f76657263656c2e636f6d2f7374617469632f7374617475732f72656164792e737667) [Ready](https://vercel.com/sui-foundation/sui-docs/4d2T9xhTE3k34D1dUEzMugTkbt5Q) | [Preview](https://sui-docs-git-amnn-cos-coin-sui-foundation.vercel.app) | [Comment](https://vercel.live/open-feedback/sui-docs-git-amnn-cos-coin-sui-foundation.vercel.app?via=pr-comment-feedback-link) | Oct 6, 2025 11:13am |  2 Skipped Deployments   | Project | Deployment | Preview | Comments | Updated (UTC) | | --- | --- | --- | --- | --- | | [multisig-toolkit](https://vercel.com/mysten-labs/multisig-toolkit) | [Ignored](https://camo.githubusercontent.com/6af980227b4c827b4b58e336e17b2523b7763da7d328256138bdddcbbe31ecbb/68747470733a2f2f76657263656c2e636f6d2f7374617469632f7374617475732f63616e63656c65642e737667) [Ignored](https://vercel.com/mysten-labs/multisig-toolkit/5ZbRT4PHDsKGvo4VUsCawKdjJJbn) |  |  | Oct 6, 2025 11:13am | | [sui-kiosk](https://vercel.com/mysten-labs/sui-kiosk) | [Ignored](https://camo.githubusercontent.com/6af980227b4c827b4b58e336e17b2523b7763da7d328256138bdddcbbe31ecbb/68747470733a2f2f76657263656c2e636f6d2f7374617469632f7374617475732f63616e63656c65642e737667) [Ignored](https://vercel.com/mysten-labs/sui-kiosk/84R3KFwTuGBmQUe9UPipESve3ueE) |  |  | Oct 6, 2025 11:13am | |

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@vercel](https://avatars.githubusercontent.com/in/8329?s=40&v=4)](/apps/vercel)
[vercel](/apps/vercel)
bot
[deployed](https://sui-docs-ghq559dhn-sui-foundation.vercel.app "Deployment has completed")
to
[Preview â€“ sui-docs](https://sui-docs-ghq559dhn-sui-foundation.vercel.app)
[October 6, 2025 11:13](#event-20112385986)
[View deployment](https://sui-docs-ghq559dhn-sui-foundation.vercel.app)

[![wlmyng](https://avatars.githubusercontent.com/u/127570466?s=60&v=4)](/wlmyng)

**[wlmyng](/wlmyng)**
approved these changes
[Oct 6, 2025](#pullrequestreview-3306505594)

[View reviewed changes](/MystenLabs/sui/pull/23851/files/4e0488940d5bed18543dce55d94048b16946ea82)

[crates/sui-indexer-alt-consistent-store/src/handlers/mod.rs](/MystenLabs/sui/pull/23851/files/4e0488940d5bed18543dce55d94048b16946ea82#diff-3461bed12b8a00a281f0a6198b60786b90385b538de11172fb223c2bf49aaf57)

|  |  |  |
| --- | --- | --- |
|  |  |  |
|  |  | let (Some(version), Some(digest)) = (change.input\_version, change.input\_digest) else { |
|  |  | // Get input version - if None, this object was created in this transaction |
|  |  | let Some(version) = change.input\_version else { |

Copy link

Contributor

### @wlmyng **[wlmyng](/wlmyng)** [Oct 6, 2025](#discussion_r2407869446)

There was a problem hiding this comment.

### Choose a reason for hiding this comment

The reason will be displayed to describe this comment to others. [Learn more](https://docs.github.com/articles/managing-disruptive-comments/#hiding-a-comment).

Choose a reason
Spam
Abuse
Off Topic
Outdated
Duplicate
Resolved
 
Hide comment

So in Effects v1, if an obj was modified, only the input version is recorded, and this was changed subsequently, but meant that we'd filter out prev vers

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

ðŸ‘
1
 amnn reacted with thumbs up emoji

All reactions

* ðŸ‘
  1 reaction

[![wlmyng](https://avatars.githubusercontent.com/u/127570466?s=60&v=4)](/wlmyng)

**[wlmyng](/wlmyng)**
reviewed
[Oct 6, 2025](#pullrequestreview-3306520249)

[View reviewed changes](/MystenLabs/sui/pull/23851/files/4e0488940d5bed18543dce55d94048b16946ea82)

[crates/sui-indexer-alt-e2e-tests/tests/consistent\_store\_list\_owned\_objects\_tests.rs](/MystenLabs/sui/pull/23851/files/4e0488940d5bed18543dce55d94048b16946ea82#diff-fc3da7fed509861e2701aeccfcd54877b1fd83f8878beece81a2fbe0ff449c2f)

Comment on lines
+603
 to 
+634

|  |  |  |
| --- | --- | --- |
|  |  | // Merge the coin into gas, changing the gas coin's balance |
|  |  | let mut builder = ProgrammableTransactionBuilder::new(); |
|  |  | builder.transfer\_sui(a, Some(500)); |
|  |  |  |
|  |  | let data = TransactionData::new\_programmable( |
|  |  | a, |
|  |  | vec![a\_gas, coin], |
|  |  | builder.finish(), |
|  |  | DEFAULT\_GAS\_BUDGET, |
|  |  | cluster.reference\_gas\_price(), |
|  |  | ); |
|  |  |  |
|  |  | let (fx, \_) = cluster |
|  |  | .execute\_transaction(Transaction::from\_data\_and\_signer(data, vec![&akp])) |
|  |  | .expect("Failed to execute transaction"); |
|  |  |  |
|  |  | assert!(fx.status().is\_ok(), "Merge transaction failed"); |
|  |  |  |
|  |  | // Verify we're testing with Effects V1 (the bug only affected V1) |
|  |  | assert!( |
|  |  | matches!(fx, TransactionEffects::V1(\_)), |
|  |  | "Test must run with Effects V1 to validate the fix" |
|  |  | ); |
|  |  |  |
|  |  | // Update gas reference |
|  |  | a\_gas = fx.gas\_object().0; |
|  |  | let new\_coin = find::address\_owned(&fx).expect("Failed to find new coin"); |
|  |  |  |
|  |  | cluster.create\_checkpoint().await; |
|  |  |  |
|  |  | // A should now own updated gas coin, the new smaller coin, but NOT the old versions |
|  |  | let objects = list\_owned\_objects(&cluster, a).await.unwrap(); |

Copy link

Contributor

### @wlmyng **[wlmyng](/wlmyng)** [Oct 6, 2025](#discussion_r2407880989)

There was a problem hiding this comment.

### Choose a reason for hiding this comment

The reason will be displayed to describe this comment to others. [Learn more](https://docs.github.com/articles/managing-disruptive-comments/#hiding-a-comment).

Choose a reason
Spam
Abuse
Off Topic
Outdated
Duplicate
Resolved
 
Hide comment

so now it's a\_gas+coin, and new coin of 500 from L605 transfer\_sui

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

ðŸ‘
1
 amnn reacted with thumbs up emoji

All reactions

* ðŸ‘
  1 reaction

Hide details
View details
[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)
[amnn](/amnn)
merged commit [`151de47`](/MystenLabs/sui/commit/151de47de5cdca81bf5fe9d438aa9a3b9f115c23)
into

main
[Oct 6, 2025](https://github.com/MystenLabs/sui/pull/23851#event-20123912710)

53 checks passed

### Uh oh!

There was an error while loading. Please reload this page.

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)
[amnn](/amnn)
deleted the

amnn/cos-coin
 
branch
[October 6, 2025 19:03](#event-20123914369)

[amnn](/amnn)
added a commit
that referenced
this pull request
[Oct 7, 2025](#ref-commit-f2ac1c2)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)

`consistency: restore from formal snapshot`

â€¦

`f2ac1c2`

```move
## Description

Initial implementation of restore from formal snapshot for the
consistent store:

- Logic for reading a formal snapshot from an object store. An
  implementation of this already exists in `sui-snapshot`, but was not
  suitable, because:
  - It is tightly coupled to (and takes dependencies on) the authority
    (fullnode/validator) codebase and its restoration process.
  - The way it configures where to fetch the snapshot from is quite
    difficult to make sense of (spread across multiple files, with
    different sources reading from the same parameters but with
    different semantics), and hardcodes certain URLs which confounds
    decentralization.
  - It makes some undesirable architectural choices, like requiring the
    formal snapshot be first downloaded and then worked over (which
    increases the storage requirements on nodes).
  - In future we may consider merging some of this new implementation
    with the old implementation.

- Additions to the `Db` abstraction to support making writes as part of
  restoration (along with a marker row to indicate a partition has been
  restored), as well as the existing support for writes with a
  watermark.
  - In future we should revisit the `Db` abstraction, and consider
    separating the notion of watermarking/snapshots/markers from the
    core abstraction over RocksDB.

- A `Restore` trait, that allows pipelines to define how to restore from
  a single `Object`. This combines processing the object with writing it
  out.
  - In future (if/when there is a need) this could be further
    generalized along the following lines:
  - Should pipelines be given access to all the `LiveObjects` in one go?
  - Should they return a list of records to write with their committer,
    rather than couple writes with processing?
  - Should the `Store`/`Connection` abstraction be extended to support
    restoration?
  - Should the `bucket`/`partition` style of sharding be generalized, so
    that indexers can also restore from some other source of live
    objects?

- A `broadcaster`/`worker` set-up, similar to the indexer, for
  parallelizing fetching parts of the formal snapshot, and restoring for
  each pipeline.
  - Automated tests for these will follow in a subsequent PR/commit.

- A generic `Restorer` that works like the `Indexer` but to coordinate
  restoration, instead of ingestion.
  - At the moment the binary either needs to be run to restore or to
    index -- you cannot make a single invocation that does both, and you
    cannot run two instances over the same DB (because the first
    instance will take a lock on the DB). This is OK for now -- where we
    want to use formal snapshots to speed up spinning up a new instance
    of the store.
  - In future, when we have an existing store and want to add a pipeline
    or data set to it, we are going to need to support indexing existing
    pipelines and bringing up a new pipeline at the same time. At that
    point, the `Restorer` may cease to be a distinct concept, instead
    being folded into the consistent store's `Indexer`.

## Test plan

There are not many automated tests with this change, more will come, but
what was added can be executed with:

```
$ cargo nextest run -p sui-indexer-alt-consistent-store
```

I have also locally tested restoring the consistent store on mainnet,
over HTTP,

```
$ cargo run -p sui-indexer-alt-consistent-store -- restore \
  --object-file-concurrency 5                              \
  --database-path /tmp/mnt                                 \
  --remote-store-url https://checkpoints.mainnet.sui.io    \
  --http https://formal-snapshot.mainnet.sui.io            \
  --pipeline balances                                      \
  --pipeline object_by_owner                               \
  --pipeline object_by_type
```

(This took about 6 hours, and the final DB weighs in at 127GiB)

Resuming indexing from the restored consistent store,
```
$ cargo run -p sui-indexer-alt-consistent-store --      \
  generate-config > /tmp/cos.toml

$ cargo run -p sui-indexer-alt-consistent-store -- run  \
  --database-path /tmp/mnt                              \
  --remote-store-url https://checkpoints.mainnet.sui.io \
  --config /tmp/cos.toml
```

querying that resumed store,

```
$ grpcurl -plaintext                                                                                              \
  -d '{"owner": { "kind": 1, "address": "0xc69be1643c03894535d653664282da5ef18ecce0c996c634d2cf012f0c66eb6f" } }' \
  localhost:7001                                                                                                  \
  sui.rpc.consistent.v1alpha.ConsistentService/ListOwnedObjects
```

and confirming that the bug fixed by #23851 is not present in the
restored index (querying the owned objects of the address above on
GraphQL Beta mainnet currently returns historical data, while the local,
restored index only returns the latest versions of all objects).
```

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)
[amnn](/amnn)
mentioned this pull request
[Oct 7, 2025](#ref-pullrequest-3491490541)

[consistency: restore from formal snapshot
#23874](/MystenLabs/sui/pull/23874)

Merged

7 tasks

[amnn](/amnn)
added a commit
that referenced
this pull request
[Oct 8, 2025](#ref-commit-0bb45b4)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)

`consistency: restore from formal snapshot`

â€¦

`0bb45b4`

```move
## Description

Initial implementation of restore from formal snapshot for the
consistent store:

- Logic for reading a formal snapshot from an object store. An
  implementation of this already exists in `sui-snapshot`, but was not
  suitable, because:
  - It is tightly coupled to (and takes dependencies on) the authority
    (fullnode/validator) codebase and its restoration process.
  - The way it configures where to fetch the snapshot from is quite
    difficult to make sense of (spread across multiple files, with
    different sources reading from the same parameters but with
    different semantics), and hardcodes certain URLs which confounds
    decentralization.
  - It makes some undesirable architectural choices, like requiring the
    formal snapshot be first downloaded and then worked over (which
    increases the storage requirements on nodes).
  - In future we may consider merging some of this new implementation
    with the old implementation.

- Additions to the `Db` abstraction to support making writes as part of
  restoration (along with a marker row to indicate a partition has been
  restored), as well as the existing support for writes with a
  watermark.
  - In future we should revisit the `Db` abstraction, and consider
    separating the notion of watermarking/snapshots/markers from the
    core abstraction over RocksDB.

- A `Restore` trait, that allows pipelines to define how to restore from
  a single `Object`. This combines processing the object with writing it
  out.
  - In future (if/when there is a need) this could be further
    generalized along the following lines:
  - Should pipelines be given access to all the `LiveObjects` in one go?
  - Should they return a list of records to write with their committer,
    rather than couple writes with processing?
  - Should the `Store`/`Connection` abstraction be extended to support
    restoration?
  - Should the `bucket`/`partition` style of sharding be generalized, so
    that indexers can also restore from some other source of live
    objects?

- A `broadcaster`/`worker` set-up, similar to the indexer, for
  parallelizing fetching parts of the formal snapshot, and restoring for
  each pipeline.
  - Automated tests for these will follow in a subsequent PR/commit.

- A generic `Restorer` that works like the `Indexer` but to coordinate
  restoration, instead of ingestion.
  - At the moment the binary either needs to be run to restore or to
    index -- you cannot make a single invocation that does both, and you
    cannot run two instances over the same DB (because the first
    instance will take a lock on the DB). This is OK for now -- where we
    want to use formal snapshots to speed up spinning up a new instance
    of the store.
  - In future, when we have an existing store and want to add a pipeline
    or data set to it, we are going to need to support indexing existing
    pipelines and bringing up a new pipeline at the same time. At that
    point, the `Restorer` may cease to be a distinct concept, instead
    being folded into the consistent store's `Indexer`.

## Test plan

There are not many automated tests with this change, more will come, but
what was added can be executed with:

```
$ cargo nextest run -p sui-indexer-alt-consistent-store
```

I have also locally tested restoring the consistent store on mainnet,
over HTTP,

```
$ cargo run -p sui-indexer-alt-consistent-store -- restore \
  --object-file-concurrency 5                              \
  --database-path /tmp/mnt                                 \
  --remote-store-url https://checkpoints.mainnet.sui.io    \
  --http https://formal-snapshot.mainnet.sui.io            \
  --pipeline balances                                      \
  --pipeline object_by_owner                               \
  --pipeline object_by_type
```

(This took about 6 hours, and the final DB weighs in at 127GiB)

Resuming indexing from the restored consistent store,
```
$ cargo run -p sui-indexer-alt-consistent-store --      \
  generate-config > /tmp/cos.toml

$ cargo run -p sui-indexer-alt-consistent-store -- run  \
  --database-path /tmp/mnt                              \
  --remote-store-url https://checkpoints.mainnet.sui.io \
  --config /tmp/cos.toml
```

querying that resumed store,

```
$ grpcurl -plaintext                                                                                              \
  -d '{"owner": { "kind": 1, "address": "0xc69be1643c03894535d653664282da5ef18ecce0c996c634d2cf012f0c66eb6f" } }' \
  localhost:7001                                                                                                  \
  sui.rpc.consistent.v1alpha.ConsistentService/ListOwnedObjects
```

and confirming that the bug fixed by #23851 is not present in the
restored index (querying the owned objects of the address above on
GraphQL Beta mainnet currently returns historical data, while the local,
restored index only returns the latest versions of all objects).
```

[amnn](/amnn)
added a commit
that referenced
this pull request
[Oct 8, 2025](#ref-commit-7a52d7a)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)

`consistency: restore from formal snapshot`

â€¦

`7a52d7a`

```move
## Description

Initial implementation of restore from formal snapshot for the
consistent store:

- Logic for reading a formal snapshot from an object store. An
  implementation of this already exists in `sui-snapshot`, but was not
  suitable, because:
  - It is tightly coupled to (and takes dependencies on) the authority
    (fullnode/validator) codebase and its restoration process.
  - The way it configures where to fetch the snapshot from is quite
    difficult to make sense of (spread across multiple files, with
    different sources reading from the same parameters but with
    different semantics), and hardcodes certain URLs which confounds
    decentralization.
  - It makes some undesirable architectural choices, like requiring the
    formal snapshot be first downloaded and then worked over (which
    increases the storage requirements on nodes).
  - In future we may consider merging some of this new implementation
    with the old implementation.

- Additions to the `Db` abstraction to support making writes as part of
  restoration (along with a marker row to indicate a partition has been
  restored), as well as the existing support for writes with a
  watermark.
  - In future we should revisit the `Db` abstraction, and consider
    separating the notion of watermarking/snapshots/markers from the
    core abstraction over RocksDB.

- A `Restore` trait, that allows pipelines to define how to restore from
  a single `Object`. This combines processing the object with writing it
  out.
  - In future (if/when there is a need) this could be further
    generalized along the following lines:
  - Should pipelines be given access to all the `LiveObjects` in one go?
  - Should they return a list of records to write with their committer,
    rather than couple writes with processing?
  - Should the `Store`/`Connection` abstraction be extended to support
    restoration?
  - Should the `bucket`/`partition` style of sharding be generalized, so
    that indexers can also restore from some other source of live
    objects?

- A `broadcaster`/`worker` set-up, similar to the indexer, for
  parallelizing fetching parts of the formal snapshot, and restoring for
  each pipeline.
  - Automated tests for these will follow in a subsequent PR/commit.

- A generic `Restorer` that works like the `Indexer` but to coordinate
  restoration, instead of ingestion.
  - At the moment the binary either needs to be run to restore or to
    index -- you cannot make a single invocation that does both, and you
    cannot run two instances over the same DB (because the first
    instance will take a lock on the DB). This is OK for now -- where we
    want to use formal snapshots to speed up spinning up a new instance
    of the store.
  - In future, when we have an existing store and want to add a pipeline
    or data set to it, we are going to need to support indexing existing
    pipelines and bringing up a new pipeline at the same time. At that
    point, the `Restorer` may cease to be a distinct concept, instead
    being folded into the consistent store's `Indexer`.

## Test plan

There are not many automated tests with this change, more will come, but
what was added can be executed with:

```
$ cargo nextest run -p sui-indexer-alt-consistent-store
```

I have also locally tested restoring the consistent store on mainnet,
over HTTP,

```
$ cargo run -p sui-indexer-alt-consistent-store -- restore \
  --object-file-concurrency 5                              \
  --database-path /tmp/mnt                                 \
  --remote-store-url https://checkpoints.mainnet.sui.io    \
  --http https://formal-snapshot.mainnet.sui.io            \
  --pipeline balances                                      \
  --pipeline object_by_owner                               \
  --pipeline object_by_type
```

(This took about 6 hours, and the final DB weighs in at 127GiB)

Resuming indexing from the restored consistent store,
```
$ cargo run -p sui-indexer-alt-consistent-store --      \
  generate-config > /tmp/cos.toml

$ cargo run -p sui-indexer-alt-consistent-store -- run  \
  --database-path /tmp/mnt                              \
  --remote-store-url https://checkpoints.mainnet.sui.io \
  --config /tmp/cos.toml
```

querying that resumed store,

```
$ grpcurl -plaintext                                                                                              \
  -d '{"owner": { "kind": 1, "address": "0xc69be1643c03894535d653664282da5ef18ecce0c996c634d2cf012f0c66eb6f" } }' \
  localhost:7001                                                                                                  \
  sui.rpc.consistent.v1alpha.ConsistentService/ListOwnedObjects
```

and confirming that the bug fixed by #23851 is not present in the
restored index (querying the owned objects of the address above on
GraphQL Beta mainnet currently returns historical data, while the local,
restored index only returns the latest versions of all objects).
```

[amnn](/amnn)
added a commit
that referenced
this pull request
[Oct 8, 2025](#ref-commit-5ae255d)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)

`consistency: restore from formal snapshot`

â€¦

`5ae255d`

```move
## Description

Initial implementation of restore from formal snapshot for the
consistent store:

- Logic for reading a formal snapshot from an object store. An
  implementation of this already exists in `sui-snapshot`, but was not
  suitable, because:
  - It is tightly coupled to (and takes dependencies on) the authority
    (fullnode/validator) codebase and its restoration process.
  - The way it configures where to fetch the snapshot from is quite
    difficult to make sense of (spread across multiple files, with
    different sources reading from the same parameters but with
    different semantics), and hardcodes certain URLs which confounds
    decentralization.
  - It makes some undesirable architectural choices, like requiring the
    formal snapshot be first downloaded and then worked over (which
    increases the storage requirements on nodes).
  - In future we may consider merging some of this new implementation
    with the old implementation.

- Additions to the `Db` abstraction to support making writes as part of
  restoration (along with a marker row to indicate a partition has been
  restored), as well as the existing support for writes with a
  watermark.
  - In future we should revisit the `Db` abstraction, and consider
    separating the notion of watermarking/snapshots/markers from the
    core abstraction over RocksDB.

- A `Restore` trait, that allows pipelines to define how to restore from
  a single `Object`. This combines processing the object with writing it
  out.
  - In future (if/when there is a need) this could be further
    generalized along the following lines:
  - Should pipelines be given access to all the `LiveObjects` in one go?
  - Should they return a list of records to write with their committer,
    rather than couple writes with processing?
  - Should the `Store`/`Connection` abstraction be extended to support
    restoration?
  - Should the `bucket`/`partition` style of sharding be generalized, so
    that indexers can also restore from some other source of live
    objects?

- A `broadcaster`/`worker` set-up, similar to the indexer, for
  parallelizing fetching parts of the formal snapshot, and restoring for
  each pipeline.
  - Automated tests for these will follow in a subsequent PR/commit.

- A generic `Restorer` that works like the `Indexer` but to coordinate
  restoration, instead of ingestion.
  - At the moment the binary either needs to be run to restore or to
    index -- you cannot make a single invocation that does both, and you
    cannot run two instances over the same DB (because the first
    instance will take a lock on the DB). This is OK for now -- where we
    want to use formal snapshots to speed up spinning up a new instance
    of the store.
  - In future, when we have an existing store and want to add a pipeline
    or data set to it, we are going to need to support indexing existing
    pipelines and bringing up a new pipeline at the same time. At that
    point, the `Restorer` may cease to be a distinct concept, instead
    being folded into the consistent store's `Indexer`.

## Test plan

There are not many automated tests with this change, more will come, but
what was added can be executed with:

```
$ cargo nextest run -p sui-indexer-alt-consistent-store
```

I have also locally tested restoring the consistent store on mainnet,
over HTTP,

```
$ cargo run -p sui-indexer-alt-consistent-store -- restore \
  --object-file-concurrency 5                              \
  --database-path /tmp/mnt                                 \
  --remote-store-url https://checkpoints.mainnet.sui.io    \
  --http https://formal-snapshot.mainnet.sui.io            \
  --pipeline balances                                      \
  --pipeline object_by_owner                               \
  --pipeline object_by_type
```

(This took about 6 hours, and the final DB weighs in at 127GiB)

Resuming indexing from the restored consistent store,
```
$ cargo run -p sui-indexer-alt-consistent-store --      \
  generate-config > /tmp/cos.toml

$ cargo run -p sui-indexer-alt-consistent-store -- run  \
  --database-path /tmp/mnt                              \
  --remote-store-url https://checkpoints.mainnet.sui.io \
  --config /tmp/cos.toml
```

querying that resumed store,

```
$ grpcurl -plaintext                                                                                              \
  -d '{"owner": { "kind": 1, "address": "0xc69be1643c03894535d653664282da5ef18ecce0c996c634d2cf012f0c66eb6f" } }' \
  localhost:7001                                                                                                  \
  sui.rpc.consistent.v1alpha.ConsistentService/ListOwnedObjects
```

and confirming that the bug fixed by #23851 is not present in the
restored index (querying the owned objects of the address above on
GraphQL Beta mainnet currently returns historical data, while the local,
restored index only returns the latest versions of all objects).
```

[amnn](/amnn)
added a commit
that referenced
this pull request
[Oct 8, 2025](#ref-commit-68e69e7)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)

`consistency: restore from formal snapshot`

â€¦

`68e69e7`

```move
## Description

Initial implementation of restore from formal snapshot for the
consistent store:

- Logic for reading a formal snapshot from an object store. An
  implementation of this already exists in `sui-snapshot`, but was not
  suitable, because:
  - It is tightly coupled to (and takes dependencies on) the authority
    (fullnode/validator) codebase and its restoration process.
  - The way it configures where to fetch the snapshot from is quite
    difficult to make sense of (spread across multiple files, with
    different sources reading from the same parameters but with
    different semantics), and hardcodes certain URLs which confounds
    decentralization.
  - It makes some undesirable architectural choices, like requiring the
    formal snapshot be first downloaded and then worked over (which
    increases the storage requirements on nodes).
  - In future we may consider merging some of this new implementation
    with the old implementation.

- Additions to the `Db` abstraction to support making writes as part of
  restoration (along with a marker row to indicate a partition has been
  restored), as well as the existing support for writes with a
  watermark.
  - In future we should revisit the `Db` abstraction, and consider
    separating the notion of watermarking/snapshots/markers from the
    core abstraction over RocksDB.

- A `Restore` trait, that allows pipelines to define how to restore from
  a single `Object`. This combines processing the object with writing it
  out.
  - In future (if/when there is a need) this could be further
    generalized along the following lines:
  - Should pipelines be given access to all the `LiveObjects` in one go?
  - Should they return a list of records to write with their committer,
    rather than couple writes with processing?
  - Should the `Store`/`Connection` abstraction be extended to support
    restoration?
  - Should the `bucket`/`partition` style of sharding be generalized, so
    that indexers can also restore from some other source of live
    objects?

- A `broadcaster`/`worker` set-up, similar to the indexer, for
  parallelizing fetching parts of the formal snapshot, and restoring for
  each pipeline.
  - Automated tests for these will follow in a subsequent PR/commit.

- A generic `Restorer` that works like the `Indexer` but to coordinate
  restoration, instead of ingestion.
  - At the moment the binary either needs to be run to restore or to
    index -- you cannot make a single invocation that does both, and you
    cannot run two instances over the same DB (because the first
    instance will take a lock on the DB). This is OK for now -- where we
    want to use formal snapshots to speed up spinning up a new instance
    of the store.
  - In future, when we have an existing store and want to add a pipeline
    or data set to it, we are going to need to support indexing existing
    pipelines and bringing up a new pipeline at the same time. At that
    point, the `Restorer` may cease to be a distinct concept, instead
    being folded into the consistent store's `Indexer`.

## Test plan

There are not many automated tests with this change, more will come, but
what was added can be executed with:

```
$ cargo nextest run -p sui-indexer-alt-consistent-store
```

I have also locally tested restoring the consistent store on mainnet,
over HTTP,

```
$ cargo run -p sui-indexer-alt-consistent-store -- restore \
  --object-file-concurrency 5                              \
  --database-path /tmp/mnt                                 \
  --remote-store-url https://checkpoints.mainnet.sui.io    \
  --http https://formal-snapshot.mainnet.sui.io            \
  --pipeline balances                                      \
  --pipeline object_by_owner                               \
  --pipeline object_by_type
```

(This took about 6 hours, and the final DB weighs in at 127GiB)

Resuming indexing from the restored consistent store,
```
$ cargo run -p sui-indexer-alt-consistent-store --      \
  generate-config > /tmp/cos.toml

$ cargo run -p sui-indexer-alt-consistent-store -- run  \
  --database-path /tmp/mnt                              \
  --remote-store-url https://checkpoints.mainnet.sui.io \
  --config /tmp/cos.toml
```

querying that resumed store,

```
$ grpcurl -plaintext                                                                                              \
  -d '{"owner": { "kind": 1, "address": "0xc69be1643c03894535d653664282da5ef18ecce0c996c634d2cf012f0c66eb6f" } }' \
  localhost:7001                                                                                                  \
  sui.rpc.consistent.v1alpha.ConsistentService/ListOwnedObjects
```

and confirming that the bug fixed by #23851 is not present in the
restored index (querying the owned objects of the address above on
GraphQL Beta mainnet currently returns historical data, while the local,
restored index only returns the latest versions of all objects).
```

[amnn](/amnn)
added a commit
that referenced
this pull request
[Oct 8, 2025](#ref-commit-018196f)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)

`consistency: restore from formal snapshot`

â€¦

`018196f`

```move
## Description

Initial implementation of restore from formal snapshot for the
consistent store:

- Logic for reading a formal snapshot from an object store. An
  implementation of this already exists in `sui-snapshot`, but was not
  suitable, because:
  - It is tightly coupled to (and takes dependencies on) the authority
    (fullnode/validator) codebase and its restoration process.
  - The way it configures where to fetch the snapshot from is quite
    difficult to make sense of (spread across multiple files, with
    different sources reading from the same parameters but with
    different semantics), and hardcodes certain URLs which confounds
    decentralization.
  - It makes some undesirable architectural choices, like requiring the
    formal snapshot be first downloaded and then worked over (which
    increases the storage requirements on nodes).
  - In future we may consider merging some of this new implementation
    with the old implementation.

- Additions to the `Db` abstraction to support making writes as part of
  restoration (along with a marker row to indicate a partition has been
  restored), as well as the existing support for writes with a
  watermark.
  - In future we should revisit the `Db` abstraction, and consider
    separating the notion of watermarking/snapshots/markers from the
    core abstraction over RocksDB.

- A `Restore` trait, that allows pipelines to define how to restore from
  a single `Object`. This combines processing the object with writing it
  out.
  - In future (if/when there is a need) this could be further
    generalized along the following lines:
  - Should pipelines be given access to all the `LiveObjects` in one go?
  - Should they return a list of records to write with their committer,
    rather than couple writes with processing?
  - Should the `Store`/`Connection` abstraction be extended to support
    restoration?
  - Should the `bucket`/`partition` style of sharding be generalized, so
    that indexers can also restore from some other source of live
    objects?

- A `broadcaster`/`worker` set-up, similar to the indexer, for
  parallelizing fetching parts of the formal snapshot, and restoring for
  each pipeline.
  - Automated tests for these will follow in a subsequent PR/commit.

- A generic `Restorer` that works like the `Indexer` but to coordinate
  restoration, instead of ingestion.
  - At the moment the binary either needs to be run to restore or to
    index -- you cannot make a single invocation that does both, and you
    cannot run two instances over the same DB (because the first
    instance will take a lock on the DB). This is OK for now -- where we
    want to use formal snapshots to speed up spinning up a new instance
    of the store.
  - In future, when we have an existing store and want to add a pipeline
    or data set to it, we are going to need to support indexing existing
    pipelines and bringing up a new pipeline at the same time. At that
    point, the `Restorer` may cease to be a distinct concept, instead
    being folded into the consistent store's `Indexer`.

## Test plan

There are not many automated tests with this change, more will come, but
what was added can be executed with:

```
$ cargo nextest run -p sui-indexer-alt-consistent-store
```

I have also locally tested restoring the consistent store on mainnet,
over HTTP,

```
$ cargo run -p sui-indexer-alt-consistent-store -- restore \
  --object-file-concurrency 5                              \
  --database-path /tmp/mnt                                 \
  --remote-store-url https://checkpoints.mainnet.sui.io    \
  --http https://formal-snapshot.mainnet.sui.io            \
  --pipeline balances                                      \
  --pipeline object_by_owner                               \
  --pipeline object_by_type
```

(This took about 6 hours, and the final DB weighs in at 127GiB)

Resuming indexing from the restored consistent store,
```
$ cargo run -p sui-indexer-alt-consistent-store --      \
  generate-config > /tmp/cos.toml

$ cargo run -p sui-indexer-alt-consistent-store -- run  \
  --database-path /tmp/mnt                              \
  --remote-store-url https://checkpoints.mainnet.sui.io \
  --config /tmp/cos.toml
```

querying that resumed store,

```
$ grpcurl -plaintext                                                                                              \
  -d '{"owner": { "kind": 1, "address": "0xc69be1643c03894535d653664282da5ef18ecce0c996c634d2cf012f0c66eb6f" } }' \
  localhost:7001                                                                                                  \
  sui.rpc.consistent.v1alpha.ConsistentService/ListOwnedObjects
```

and confirming that the bug fixed by #23851 is not present in the
restored index (querying the owned objects of the address above on
GraphQL Beta mainnet currently returns historical data, while the local,
restored index only returns the latest versions of all objects).
```

[amnn](/amnn)
added a commit
that referenced
this pull request
[Oct 8, 2025](#ref-commit-de93177)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)

`consistency: restore from formal snapshot`

â€¦

`de93177`

```move
## Description

Initial implementation of restore from formal snapshot for the
consistent store:

- Logic for reading a formal snapshot from an object store. An
  implementation of this already exists in `sui-snapshot`, but was not
  suitable, because:
  - It is tightly coupled to (and takes dependencies on) the authority
    (fullnode/validator) codebase and its restoration process.
  - The way it configures where to fetch the snapshot from is quite
    difficult to make sense of (spread across multiple files, with
    different sources reading from the same parameters but with
    different semantics), and hardcodes certain URLs which confounds
    decentralization.
  - It makes some undesirable architectural choices, like requiring the
    formal snapshot be first downloaded and then worked over (which
    increases the storage requirements on nodes).
  - In future we may consider merging some of this new implementation
    with the old implementation.

- Additions to the `Db` abstraction to support making writes as part of
  restoration (along with a marker row to indicate a partition has been
  restored), as well as the existing support for writes with a
  watermark.
  - In future we should revisit the `Db` abstraction, and consider
    separating the notion of watermarking/snapshots/markers from the
    core abstraction over RocksDB.

- A `Restore` trait, that allows pipelines to define how to restore from
  a single `Object`. This combines processing the object with writing it
  out.
  - In future (if/when there is a need) this could be further
    generalized along the following lines:
  - Should pipelines be given access to all the `LiveObjects` in one go?
  - Should they return a list of records to write with their committer,
    rather than couple writes with processing?
  - Should the `Store`/`Connection` abstraction be extended to support
    restoration?
  - Should the `bucket`/`partition` style of sharding be generalized, so
    that indexers can also restore from some other source of live
    objects?

- A `broadcaster`/`worker` set-up, similar to the indexer, for
  parallelizing fetching parts of the formal snapshot, and restoring for
  each pipeline.
  - Automated tests for these will follow in a subsequent PR/commit.

- A generic `Restorer` that works like the `Indexer` but to coordinate
  restoration, instead of ingestion.
  - At the moment the binary either needs to be run to restore or to
    index -- you cannot make a single invocation that does both, and you
    cannot run two instances over the same DB (because the first
    instance will take a lock on the DB). This is OK for now -- where we
    want to use formal snapshots to speed up spinning up a new instance
    of the store.
  - In future, when we have an existing store and want to add a pipeline
    or data set to it, we are going to need to support indexing existing
    pipelines and bringing up a new pipeline at the same time. At that
    point, the `Restorer` may cease to be a distinct concept, instead
    being folded into the consistent store's `Indexer`.

## Test plan

There are not many automated tests with this change, more will come, but
what was added can be executed with:

```
$ cargo nextest run -p sui-indexer-alt-consistent-store
```

I have also locally tested restoring the consistent store on mainnet,
over HTTP,

```
$ cargo run -p sui-indexer-alt-consistent-store -- restore \
  --object-file-concurrency 5                              \
  --database-path /tmp/mnt                                 \
  --remote-store-url https://checkpoints.mainnet.sui.io    \
  --http https://formal-snapshot.mainnet.sui.io            \
  --pipeline balances                                      \
  --pipeline object_by_owner                               \
  --pipeline object_by_type
```

(This took about 6 hours, and the final DB weighs in at 127GiB)

Resuming indexing from the restored consistent store,
```
$ cargo run -p sui-indexer-alt-consistent-store --      \
  generate-config > /tmp/cos.toml

$ cargo run -p sui-indexer-alt-consistent-store -- run  \
  --database-path /tmp/mnt                              \
  --remote-store-url https://checkpoints.mainnet.sui.io \
  --config /tmp/cos.toml
```

querying that resumed store,

```
$ grpcurl -plaintext                                                                                              \
  -d '{"owner": { "kind": 1, "address": "0xc69be1643c03894535d653664282da5ef18ecce0c996c634d2cf012f0c66eb6f" } }' \
  localhost:7001                                                                                                  \
  sui.rpc.consistent.v1alpha.ConsistentService/ListOwnedObjects
```

and confirming that the bug fixed by #23851 is not present in the
restored index (querying the owned objects of the address above on
GraphQL Beta mainnet currently returns historical data, while the local,
restored index only returns the latest versions of all objects).
```

[amnn](/amnn)
added a commit
that referenced
this pull request
[Oct 8, 2025](#ref-commit-044bfee)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)

`consistency: restore from formal snapshot`

â€¦

`044bfee`

```move
## Description

Initial implementation of restore from formal snapshot for the
consistent store:

- Logic for reading a formal snapshot from an object store. An
  implementation of this already exists in `sui-snapshot`, but was not
  suitable, because:
  - It is tightly coupled to (and takes dependencies on) the authority
    (fullnode/validator) codebase and its restoration process.
  - The way it configures where to fetch the snapshot from is quite
    difficult to make sense of (spread across multiple files, with
    different sources reading from the same parameters but with
    different semantics), and hardcodes certain URLs which confounds
    decentralization.
  - It makes some undesirable architectural choices, like requiring the
    formal snapshot be first downloaded and then worked over (which
    increases the storage requirements on nodes).
  - In future we may consider merging some of this new implementation
    with the old implementation.

- Additions to the `Db` abstraction to support making writes as part of
  restoration (along with a marker row to indicate a partition has been
  restored), as well as the existing support for writes with a
  watermark.
  - In future we should revisit the `Db` abstraction, and consider
    separating the notion of watermarking/snapshots/markers from the
    core abstraction over RocksDB.

- A `Restore` trait, that allows pipelines to define how to restore from
  a single `Object`. This combines processing the object with writing it
  out.
  - In future (if/when there is a need) this could be further
    generalized along the following lines:
  - Should pipelines be given access to all the `LiveObjects` in one go?
  - Should they return a list of records to write with their committer,
    rather than couple writes with processing?
  - Should the `Store`/`Connection` abstraction be extended to support
    restoration?
  - Should the `bucket`/`partition` style of sharding be generalized, so
    that indexers can also restore from some other source of live
    objects?

- A `broadcaster`/`worker` set-up, similar to the indexer, for
  parallelizing fetching parts of the formal snapshot, and restoring for
  each pipeline.
  - Automated tests for these will follow in a subsequent PR/commit.

- A generic `Restorer` that works like the `Indexer` but to coordinate
  restoration, instead of ingestion.
  - At the moment the binary either needs to be run to restore or to
    index -- you cannot make a single invocation that does both, and you
    cannot run two instances over the same DB (because the first
    instance will take a lock on the DB). This is OK for now -- where we
    want to use formal snapshots to speed up spinning up a new instance
    of the store.
  - In future, when we have an existing store and want to add a pipeline
    or data set to it, we are going to need to support indexing existing
    pipelines and bringing up a new pipeline at the same time. At that
    point, the `Restorer` may cease to be a distinct concept, instead
    being folded into the consistent store's `Indexer`.

## Test plan

There are not many automated tests with this change, more will come, but
what was added can be executed with:

```
$ cargo nextest run -p sui-indexer-alt-consistent-store
```

I have also locally tested restoring the consistent store on mainnet,
over HTTP,

```
$ cargo run -p sui-indexer-alt-consistent-store -- restore \
  --object-file-concurrency 5                              \
  --database-path /tmp/mnt                                 \
  --remote-store-url https://checkpoints.mainnet.sui.io    \
  --http https://formal-snapshot.mainnet.sui.io            \
  --pipeline balances                                      \
  --pipeline object_by_owner                               \
  --pipeline object_by_type
```

(This took about 6 hours, and the final DB weighs in at 127GiB)

Resuming indexing from the restored consistent store,
```
$ cargo run -p sui-indexer-alt-consistent-store --      \
  generate-config > /tmp/cos.toml

$ cargo run -p sui-indexer-alt-consistent-store -- run  \
  --database-path /tmp/mnt                              \
  --remote-store-url https://checkpoints.mainnet.sui.io \
  --config /tmp/cos.toml
```

querying that resumed store,

```
$ grpcurl -plaintext                                                                                              \
  -d '{"owner": { "kind": 1, "address": "0xc69be1643c03894535d653664282da5ef18ecce0c996c634d2cf012f0c66eb6f" } }' \
  localhost:7001                                                                                                  \
  sui.rpc.consistent.v1alpha.ConsistentService/ListOwnedObjects
```

and confirming that the bug fixed by #23851 is not present in the
restored index (querying the owned objects of the address above on
GraphQL Beta mainnet currently returns historical data, while the local,
restored index only returns the latest versions of all objects).
```

[amnn](/amnn)
added a commit
that referenced
this pull request
[Oct 8, 2025](#ref-commit-815fd91)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)

`consistency: restore from formal snapshot`

â€¦

`815fd91`

```move
## Description

Initial implementation of restore from formal snapshot for the
consistent store:

- Logic for reading a formal snapshot from an object store. An
  implementation of this already exists in `sui-snapshot`, but was not
  suitable, because:
  - It is tightly coupled to (and takes dependencies on) the authority
    (fullnode/validator) codebase and its restoration process.
  - The way it configures where to fetch the snapshot from is quite
    difficult to make sense of (spread across multiple files, with
    different sources reading from the same parameters but with
    different semantics), and hardcodes certain URLs which confounds
    decentralization.
  - It makes some undesirable architectural choices, like requiring the
    formal snapshot be first downloaded and then worked over (which
    increases the storage requirements on nodes).
  - In future we may consider merging some of this new implementation
    with the old implementation.

- Additions to the `Db` abstraction to support making writes as part of
  restoration (along with a marker row to indicate a partition has been
  restored), as well as the existing support for writes with a
  watermark.
  - In future we should revisit the `Db` abstraction, and consider
    separating the notion of watermarking/snapshots/markers from the
    core abstraction over RocksDB.

- A `Restore` trait, that allows pipelines to define how to restore from
  a single `Object`. This combines processing the object with writing it
  out.
  - In future (if/when there is a need) this could be further
    generalized along the following lines:
  - Should pipelines be given access to all the `LiveObjects` in one go?
  - Should they return a list of records to write with their committer,
    rather than couple writes with processing?
  - Should the `Store`/`Connection` abstraction be extended to support
    restoration?
  - Should the `bucket`/`partition` style of sharding be generalized, so
    that indexers can also restore from some other source of live
    objects?

- A `broadcaster`/`worker` set-up, similar to the indexer, for
  parallelizing fetching parts of the formal snapshot, and restoring for
  each pipeline.
  - Automated tests for these will follow in a subsequent PR/commit.

- A generic `Restorer` that works like the `Indexer` but to coordinate
  restoration, instead of ingestion.
  - At the moment the binary either needs to be run to restore or to
    index -- you cannot make a single invocation that does both, and you
    cannot run two instances over the same DB (because the first
    instance will take a lock on the DB). This is OK for now -- where we
    want to use formal snapshots to speed up spinning up a new instance
    of the store.
  - In future, when we have an existing store and want to add a pipeline
    or data set to it, we are going to need to support indexing existing
    pipelines and bringing up a new pipeline at the same time. At that
    point, the `Restorer` may cease to be a distinct concept, instead
    being folded into the consistent store's `Indexer`.

## Test plan

There are not many automated tests with this change, more will come, but
what was added can be executed with:

```
$ cargo nextest run -p sui-indexer-alt-consistent-store
```

I have also locally tested restoring the consistent store on mainnet,
over HTTP,

```
$ cargo run -p sui-indexer-alt-consistent-store -- restore \
  --object-file-concurrency 5                              \
  --database-path /tmp/mnt                                 \
  --remote-store-url https://checkpoints.mainnet.sui.io    \
  --http https://formal-snapshot.mainnet.sui.io            \
  --pipeline balances                                      \
  --pipeline object_by_owner                               \
  --pipeline object_by_type
```

(This took about 6 hours, and the final DB weighs in at 127GiB)

Resuming indexing from the restored consistent store,
```
$ cargo run -p sui-indexer-alt-consistent-store --      \
  generate-config > /tmp/cos.toml

$ cargo run -p sui-indexer-alt-consistent-store -- run  \
  --database-path /tmp/mnt                              \
  --remote-store-url https://checkpoints.mainnet.sui.io \
  --config /tmp/cos.toml
```

querying that resumed store,

```
$ grpcurl -plaintext                                                                                              \
  -d '{"owner": { "kind": 1, "address": "0xc69be1643c03894535d653664282da5ef18ecce0c996c634d2cf012f0c66eb6f" } }' \
  localhost:7001                                                                                                  \
  sui.rpc.consistent.v1alpha.ConsistentService/ListOwnedObjects
```

and confirming that the bug fixed by #23851 is not present in the
restored index (querying the owned objects of the address above on
GraphQL Beta mainnet currently returns historical data, while the local,
restored index only returns the latest versions of all objects).
```

[amnn](/amnn)
added a commit
that referenced
this pull request
[Oct 8, 2025](#ref-commit-37fceb3)

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&u=fbf218c7e4a657c413a08873087b3f79c535da22&v=4)](/amnn)

`consistency: restore from formal snapshot`

â€¦

`37fceb3`

```move
## Description

Initial implementation of restore from formal snapshot for the
consistent store:

- Logic for reading a formal snapshot from an object store. An
  implementation of this already exists in `sui-snapshot`, but was not
  suitable, because:
  - It is tightly coupled to (and takes dependencies on) the authority
    (fullnode/validator) codebase and its restoration process.
  - The way it configures where to fetch the snapshot from is quite
    difficult to make sense of (spread across multiple files, with
    different sources reading from the same parameters but with
    different semantics), and hardcodes certain URLs which confounds
    decentralization.
  - It makes some undesirable architectural choices, like requiring the
    formal snapshot be first downloaded and then worked over (which
    increases the storage requirements on nodes).
  - In future we may consider merging some of this new implementation
    with the old implementation.

- Additions to the `Db` abstraction to support making writes as part of
  restoration (along with a marker row to indicate a partition has been
  restored), as well as the existing support for writes with a
  watermark.
  - In future we should revisit the `Db` abstraction, and consider
    separating the notion of watermarking/snapshots/markers from the
    core abstraction over RocksDB.

- A `Restore` trait, that allows pipelines to define how to restore from
  a single `Object`. This combines processing the object with writing it
  out.
  - In future (if/when there is a need) this could be further
    generalized along the following lines:
  - Should pipelines be given access to all the `LiveObjects` in one go?
  - Should they return a list of records to write with their committer,
    rather than couple writes with processing?
  - Should the `Store`/`Connection` abstraction be extended to support
    restoration?
  - Should the `bucket`/`partition` style of sharding be generalized, so
    that indexers can also restore from some other source of live
    objects?

- A `broadcaster`/`worker` set-up, similar to the indexer, for
  parallelizing fetching parts of the formal snapshot, and restoring for
  each pipeline.
  - Automated tests for these will follow in a subsequent PR/commit.

- A generic `Restorer` that works like the `Indexer` but to coordinate
  restoration, instead of ingestion.
  - At the moment the binary either needs to be run to restore or to
    index -- you cannot make a single invocation that does both, and you
    cannot run two instances over the same DB (because the first
    instance will take a lock on the DB). This is OK for now -- where we
    want to use formal snapshots to speed up spinning up a new instance
    of the store.
  - In future, when we have an existing store and want to add a pipeline
    or data set to it, we are going to need to support indexing existing
    pipelines and bringing up a new pipeline at the same time. At that
    point, the `Restorer` may cease to be a distinct concept, instead
    being folded into the consistent store's `Indexer`.

## Test plan

There are not many automated tests with this change, more will come, but
what was added can be executed with:

```
$ cargo nextest run -p sui-indexer-alt-consistent-store
```

I have also locally tested restoring the consistent store on mainnet,
over HTTP,

```
$ cargo run -p sui-indexer-alt-consistent-store -- restore \
  --object-file-concurrency 5                              \
  --database-path /tmp/mnt                                 \
  --remote-store-url https://checkpoints.mainnet.sui.io    \
  --http https://formal-snapshot.mainnet.sui.io            \
  --pipeline balances                                      \
  --pipeline object_by_owner                               \
  --pipeline object_by_type
```

(This took about 6 hours, and the final DB weighs in at 127GiB)

Resuming indexing from the restored consistent store,
```
$ cargo run -p sui-indexer-alt-consistent-store --      \
  generate-config > /tmp/cos.toml

$ cargo run -p sui-indexer-alt-consistent-store -- run  \
  --database-path /tmp/mnt                              \
  --remote-store-url https://checkpoints.mainnet.sui.io \
  --config /tmp/cos.toml
```

querying that resumed store,

```
$ grpcurl -plaintext                                                                                              \
  -d '{"owner": { "kind": 1, "address": "0xc69be1643c03894535d653664282da5ef18ecce0c996c634d2cf012f0c66eb6f" } }' \
  localhost:7001                                                                                                  \
  sui.rpc.consistent.v1alpha.ConsistentService/ListOwnedObjects
```

and confirming that the bug fixed by #23851 is not present in the
restored index (querying the owned objects of the address above on
GraphQL Beta mainnet currently returns historical data, while the local,
restored index only returns the latest versions of all objects).
```

[Sign up for free](/join?source=comment-repo)
**to join this conversation on GitHub**.
Already have an account?
[Sign in to comment](/login?return_to=https%3A%2F%2Fgithub.com%2FMystenLabs%2Fsui%2Fpull%2F23851)

### Reviewers

[![@wlmyng](https://avatars.githubusercontent.com/u/127570466?s=40&v=4)](/wlmyng) [wlmyng](/wlmyng)

wlmyng approved these changes

[![@henryachen](https://avatars.githubusercontent.com/u/8963934?s=40&v=4)](/henryachen) [henryachen](/henryachen)






 Awaiting requested review from henryachen






 henryachen is a code owner automatically assigned from MystenLabs/devx-indexers-and-rpcs

[![@emmazzz](https://avatars.githubusercontent.com/u/14351986?s=40&v=4)](/emmazzz) [emmazzz](/emmazzz)






 Awaiting requested review from emmazzz






 emmazzz is a code owner automatically assigned from MystenLabs/devx-indexers-and-rpcs

[![@tpham-mysten](https://avatars.githubusercontent.com/u/214580142?s=40&v=4)](/tpham-mysten) [tpham-mysten](/tpham-mysten)






 Awaiting requested review from tpham-mysten






 tpham-mysten is a code owner automatically assigned from MystenLabs/devx-indexers-and-rpcs

[![@evan-wall-mysten](https://avatars.githubusercontent.com/u/219740240?s=40&v=4)](/evan-wall-mysten) [evan-wall-mysten](/evan-wall-mysten)






 Awaiting requested review from evan-wall-mysten






 evan-wall-mysten is a code owner automatically assigned from MystenLabs/devx-indexers-and-rpcs

### Assignees

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=40&v=4)](/amnn) [amnn](/amnn)

### Labels

None yet

### Projects

None yet

### Milestone

No milestone

### Development

Successfully merging this pull request may close these issues.

### Uh oh!

There was an error while loading. Please reload this page.

### 3 participants

[![@amnn](https://avatars.githubusercontent.com/u/332275?s=52&v=4)](/amnn) [![@wlmyng](https://avatars.githubusercontent.com/u/127570466?s=52&v=4)](/wlmyng)

Add this suggestion to a batch that can be applied as a single commit.
This suggestion is invalid because no changes were made to the code.
Suggestions cannot be applied while the pull request is closed.
Suggestions cannot be applied while viewing a subset of changes.
Only one suggestion per line can be applied in a batch.
Add this suggestion to a batch that can be applied as a single commit.
Applying suggestions on deleted lines is not supported.
You must change the existing code in this line in order to create a valid suggestion.
Outdated suggestions cannot be applied.
This suggestion has been applied or marked resolved.
Suggestions cannot be applied from pending reviews.
Suggestions cannot be applied on multi-line comments.
Suggestions cannot be applied while the pull request is queued to merge.
Suggestion cannot be applied right now. Please check back later.

You canâ€™t perform that action at this time.